{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IAAC - 1 - Latent Space.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfke20gNnvde"
      },
      "source": [
        "# Network initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9EZXi4J1A6r9"
      },
      "source": [
        "#@title ▶ Download the neural networks from github and the training for generation faces\n",
        "\n",
        "#@markdown We will use Styelgan2-ADA to generate faces and at the end we will use CLIP to search inside the latent space\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/pbaylies/stylegan2-ada-pytorch.git\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!cp -R CLIP/clip/ stylegan2-ada-pytorch\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "%mkdir weights\n",
        "%cd weights\n",
        "!wget -nc https://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MSWo-82yBl88"
      },
      "source": [
        "#@title ▶ Install the required libraries\n",
        "\n",
        "#@markdown It may take a while (3 minutes)\n",
        "\n",
        "!apt install -y -q ninja-build\n",
        "!pip -q install mediapy opensimplex ftfy ninja\n",
        "\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu117\"\n",
        "\n",
        "!pip -q install torch==1.13.1{torch_version_suffix} torchvision==0.14.1{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXuO0sLpnq4L",
        "cellView": "form"
      },
      "source": [
        "#@title ▶ Initialize the network and define some general functions\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "# General imports\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import PIL.Image\n",
        "import random\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import os.path\n",
        "import copy\n",
        "\n",
        "# Notebook imports\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "import mediapy as media\n",
        "\n",
        "# Neural network imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Imports from Network\n",
        "import dnnlib\n",
        "import legacy\n",
        "from generate import seeds_to_zs, line_interpolate\n",
        "import clip\n",
        "import apply_factor\n",
        "\n",
        "\n",
        "custom = False\n",
        "network_pkl = 'weights/stylegan2-ffhq-config-f.pkl'\n",
        "\n",
        "G_kwargs = dnnlib.EasyDict()\n",
        "#G_kwargs.size = size\n",
        "#G_kwargs.scale_type = scale_type\n",
        "\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "    # G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "    G = legacy.load_network_pkl(f, custom=custom, **G_kwargs)['G_ema'].to(device) # type: ignore\n",
        "\n",
        "label = torch.zeros([1, G.c_dim], device=device)\n",
        "\n",
        "def generate_image(z, truncation_psi, to_pil=True):\n",
        "  zt = torch.from_numpy(z).to(device)\n",
        "  img = G(zt, label, truncation_psi=truncation_psi, noise_mode='const')\n",
        "  return torch_to_image(img, to_pil)\n",
        "\n",
        "def torch_to_image(tensor, to_pil=True):\n",
        "  img = (tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "  if to_pil:\n",
        "    return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')\n",
        "  else:\n",
        "    return img[0].cpu().numpy()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBWAISb1Mv_g"
      },
      "source": [
        "# Generating images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQhp7vKvsi1i"
      },
      "source": [
        "#@title 🖼 Generate an image { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "#@markdown 💬 A seed is a shortcut to a point in the latent space so there's no need to write a value for each of the 512 dimensions.\n",
        "seed = 1#@param {type:\"integer\"}\n",
        "#@markdown 💬 Truncation PSI is a parameter of the network that defines how \"strange\" are the images that it generates. \"Strange\" meaning away from to the \"average\" \"intermediate\".\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "z = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "im = generate_image(z, truncation_psi)\n",
        "display(im.resize((512, 512)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147KAkqvkt1y"
      },
      "source": [
        "#@title 🖼 Generate multiple images { display-mode: \"form\" }\n",
        "\n",
        "#@markdown 💬 Select the number of images and the start seed. The images that will be generated will be for the seeds start_seed to start_seed + num_seeds.\n",
        "start_seed = 1#@param {type:\"integer\"}\n",
        "num_seeds = 64#@param {type:\"integer\"}\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown 💬 Display size for each image.\n",
        "image_size = 128#@param [32, 64, 128, 256, 512]\n",
        "\n",
        "image_size = int(image_size)\n",
        "\n",
        "seeds = [seed for seed in range(start_seed, start_seed+num_seeds+1)]\n",
        "seeds_text = [str(seed) for seed in seeds]\n",
        "\n",
        "points = seeds_to_zs(G, seeds)\n",
        "\n",
        "images = []\n",
        "for point in tqdm(points, leave=False):\n",
        "  im = generate_image(point, truncation_psi, False)\n",
        "  images.append(cv2.resize(im, dsize=(image_size, image_size), interpolation=cv2.INTER_CUBIC));\n",
        "\n",
        "columns = int(1536/image_size)\n",
        "media.show_images(images, border=True, height=image_size, columns=columns, titles=seeds_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLvHLEU5M3OH"
      },
      "source": [
        "# Navigating the latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA6y2FIZtlas"
      },
      "source": [
        "#@title 🖼 Generate images from points close to one seed { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "#@markdown By generating images to points close to one seed we see that they are similar. Like generating the image at [1,1] and the imatges at [1,1.01], [1,1.02], ...\n",
        "\n",
        "def generate_variations(seed, num_variations):\n",
        "  # print(G.z_dim)\n",
        "  z = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "  # print(z)\n",
        "  im = generate_image(z, truncation_psi)\n",
        "\n",
        "  images = [im]\n",
        "\n",
        "  for i in range(0,num_variations):\n",
        "    z[0,i] = 1\n",
        "    im = generate_image(z, truncation_psi)\n",
        "    images.append(im)\n",
        "\n",
        "  media.show_images(images, border=True, height=128)\n",
        "\n",
        "seed = 1#@param {type:\"integer\"}\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "generate_variations(seed, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1SM7baF2kQY"
      },
      "source": [
        "#@title 🖼 Generate an interpolation video between two seeds { display-mode: \"form\" }\n",
        "\n",
        "#@markdown We can also generate images for the points that are between two seeds, and the convert it to a video.\n",
        "\n",
        "def generate_interpolation(seed_1, seed_2, frames, easing, video_size):\n",
        "  points = seeds_to_zs(G,[seed_1,seed_2])\n",
        "  points = line_interpolate(points,frames,easing)\n",
        "\n",
        "  images = []\n",
        "  for idx, point in enumerate(tqdm(points, leave=False)):\n",
        "    im = generate_image(point, truncation_psi, False)\n",
        "    images.append(cv2.resize(im, dsize=(video_size, video_size), interpolation=cv2.INTER_CUBIC));\n",
        "\n",
        "  media.show_video(images, fps=8)\n",
        "\n",
        "#@markdown 💬 Starting and ending seed\n",
        "seed1 = 1#@param {type:\"integer\"}\n",
        "seed2 = 2#@param {type:\"integer\"}\n",
        "truncation_psi = 1#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown 💬 Number of frames of the video: more frames takes longer and the transition is slower\n",
        "frames = 100#@param {type:\"integer\"}\n",
        "#@markdown 💬 Dimensions of the generated video\n",
        "video_size = \"512\"#@param [256, 512, 1024]\n",
        "\n",
        "#@markdown ☝ The video can be downloaded right clicking in it\n",
        "\n",
        "generate_interpolation(seed1,seed2,frames, 'linear', int(video_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBJlVQVv7RTn"
      },
      "source": [
        "#@title 🖼 Generate an interpolation matrix between four seeds { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Visualize the interpolation four-side\n",
        "\n",
        "def generate_matrix(seed_1, seed_2, seed_3, seed_4, steps, easing, truncation_psi=0.8, image_size=64):\n",
        "  points = seeds_to_zs(G,[seed_1,seed_2,seed_3,seed_4])\n",
        "  a_to_b = line_interpolate([points[0], points[1]], steps, easing)\n",
        "  c_to_d = line_interpolate([points[2], points[3]], steps, easing)\n",
        "\n",
        "  images = []\n",
        "\n",
        "  for step in range(0, steps):\n",
        "    row_points = line_interpolate([a_to_b[step], c_to_d[step]], steps, easing)\n",
        "    for point in row_points:\n",
        "      im = generate_image(point, truncation_psi, False)\n",
        "      images.append(cv2.resize(im, dsize=(256, 256), interpolation=cv2.INTER_CUBIC));\n",
        "\n",
        "  media.show_images(images, border=True, height=64, columns=steps)\n",
        "\n",
        "#@markdown 💬 Seeds at the corners\n",
        "seed1 = 1#@param {type:\"integer\"}\n",
        "seed2 = 2#@param {type:\"integer\"}\n",
        "seed3 = 3#@param {type:\"integer\"}\n",
        "seed4 = 4#@param {type:\"integer\"}\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "#@markdown 💬 Number and size of the interpolation images\n",
        "steps = 8#@param {type:\"integer\"}\n",
        "image_size = \"64\"#@param [64, 128, 256]\n",
        "\n",
        "generate_matrix(seed1, seed2, seed3, seed4, steps, 'linear', truncation_psi, int(image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usim2M07NFmU"
      },
      "source": [
        "# Trying to make sense of the directions in this space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kykdtt9BqfM"
      },
      "source": [
        "#@title ▶ Extracting the directions and defining necessary functions { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Using a command from the network a new file is created that contains some directions of the latents space that we will later use.\n",
        "\n",
        "#@markdown Also, some functions to work with the directions are defined.\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "! python closed_form_factorization.py --ckpt {network_pkl} --out {network_pkl}.pt\n",
        "\n",
        "def generate_factorized_images(z_w, label, truncation_psi, noise_mode, direction, space):\n",
        "    if(space == 'w'):\n",
        "        img1 = G.synthesis(z_w, noise_mode=noise_mode, force_fp32=True)\n",
        "        img2 = G.synthesis(z_w + direction, noise_mode=noise_mode, force_fp32=True)\n",
        "        img3 = G.synthesis(z_w - direction, noise_mode=noise_mode, force_fp32=True)\n",
        "    else:\n",
        "        img1 = G(z_w, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "        img2 = G(z_w + direction, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "        img3 = G(z_w - direction, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "\n",
        "    return [torch_to_image(img3), torch_to_image(img1), torch_to_image(img2)]\n",
        "\n",
        "\n",
        "def factorize(latents, space, index, degree, truncation_psi=0.8, image_size=128):\n",
        "\n",
        "  device = torch.device('cuda')\n",
        "  eigvec = torch.load(network_pkl + \".pt\")[\"eigvec\"].to(device)\n",
        "\n",
        "  custom = False\n",
        "\n",
        "  label = torch.zeros([1, G.c_dim], device=device) # assume no class label\n",
        "  noise_mode = \"const\" # default\n",
        "\n",
        "  index_list_of_eigenvalues = []\n",
        "\n",
        "  image_grid_eigvec = [[],[],[]]\n",
        "\n",
        "  for l in latents:\n",
        "      if len(index) ==  1 and index[0] == -1: # use all eigenvalues\n",
        "          index_list_of_eigenvalues = [*range(len(eigvec))]\n",
        "      else: # use certain indexes as eigenvalues\n",
        "          index_list_of_eigenvalues = index\n",
        "\n",
        "      for j in index_list_of_eigenvalues:\n",
        "          current_eigvec = eigvec[:, j].unsqueeze(0)\n",
        "          direction = degree * current_eigvec\n",
        "          image_group = generate_factorized_images(l, label, truncation_psi, noise_mode, direction, space)\n",
        "\n",
        "          image_grid_eigvec[0].append(image_group[0])\n",
        "          image_grid_eigvec[1].append(image_group[1])\n",
        "          image_grid_eigvec[2].append(image_group[2])\n",
        "\n",
        "          # media.show_images(image_group, border=False, height=128)\n",
        "\n",
        "  columns = len(latents)\n",
        "  if len(latents) == 1:\n",
        "    columns = 3\n",
        "  media.show_images(image_grid_eigvec[0] + image_grid_eigvec[1] + image_grid_eigvec[2], border=False, height=image_size, columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqimzzU_479W"
      },
      "source": [
        "#@title 🖼 Modify some seeds in one of the found directions { display-mode: \"form\" }\n",
        "\n",
        "#@markdown In the previous step the tool defined a list of semantic directions, now we can modify the seeds in those directions. We don't know what each direction means, so we have to try for each one.\n",
        "\n",
        "#@markdown The tool will generate the seed image and the seed image modified in the positive and negative direction for 8 seeds.\n",
        "\n",
        "#@markdown 💬 Index of the direction\n",
        "factor_index = 1#@param {type:\"integer\"}\n",
        "#@markdown 💬 Amount of change\n",
        "factor_degree = 2.5#@param {type:\"slider\", min:0, max:10, step:0.25}\n",
        "\n",
        "#@markdown &nbsp;\n",
        "start_seed = 1#@param {type:\"integer\"}\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "image_size = \"128\"#@param [64, 128, 256, 512]\n",
        "\n",
        "factor_seeds = [start_seed + i for i in range(8)]\n",
        "factor_index = [factor_index]\n",
        "\n",
        "latents_z = []\n",
        "for seed in factor_seeds:\n",
        "  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n",
        "  latents_z.append(z)\n",
        "\n",
        "latents_w = apply_factor.zs_to_ws(G,torch.device('cuda'),label,truncation_psi,latents_z)\n",
        "\n",
        "factorize(latents_w, 'w', factor_index, factor_degree, truncation_psi, int(image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOcbRyTMNOi4"
      },
      "source": [
        "# Finding you inside the latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQaOvW2gMNzm",
        "cellView": "form"
      },
      "source": [
        "#@title ⬆ Upload a picture\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!mkdir /content/images\n",
        "%cd /content/images\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('Uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "%cd /content/stylegan2-ada-pytorch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFTWhBSXm-rm"
      },
      "source": [
        "#@title ▶ Define some functions to crop and align the protrait { display-mode: \"form\" }\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "!wget -nc http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bunzip2 --keep shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "import dlib\n",
        "\n",
        "predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "def get_landmark(filepath):\n",
        "    \"\"\"get landmark with dlib\n",
        "    :return: np.array shape=(68, 2)\n",
        "    \"\"\"\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    img = dlib.load_rgb_image(filepath)\n",
        "    dets = detector(img, 1)\n",
        "\n",
        "    print(\"Number of faces detected: {}\".format(len(dets)))\n",
        "    for k, d in enumerate(dets):\n",
        "        print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n",
        "            k, d.left(), d.top(), d.right(), d.bottom()))\n",
        "        # Get the landmarks/parts for the face in box d.\n",
        "        shape = predictor(img, d)\n",
        "        print(\"Part 0: {}, Part 1: {} ...\".format(shape.part(0), shape.part(1)))\n",
        "\n",
        "\n",
        "    t = list(shape.parts())\n",
        "    a = []\n",
        "    for tt in t:\n",
        "        a.append([tt.x, tt.y])\n",
        "    lm = np.array(a)\n",
        "    # lm is a shape=(68,2) np.array\n",
        "    return lm\n",
        "\n",
        "\n",
        "def align_face(filepath):\n",
        "    \"\"\"\n",
        "    :param filepath: str\n",
        "    :return: PIL Image\n",
        "    \"\"\"\n",
        "\n",
        "    lm = get_landmark(filepath)\n",
        "\n",
        "    lm_chin          = lm[0  : 17]  # left-right\n",
        "    lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "    lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "    lm_nose          = lm[27 : 31]  # top-down\n",
        "    lm_nostrils      = lm[31 : 36]  # top-down\n",
        "    lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "    lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "    lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "    lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "    # Calculate auxiliary vectors.\n",
        "    eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "    eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "    eye_avg      = (eye_left + eye_right) * 0.5\n",
        "    eye_to_eye   = eye_right - eye_left\n",
        "    mouth_left   = lm_mouth_outer[0]\n",
        "    mouth_right  = lm_mouth_outer[6]\n",
        "    mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "    eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_avg + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "\n",
        "    # read image\n",
        "    img = PIL.Image.open(filepath)\n",
        "\n",
        "    output_size=1024\n",
        "    transform_size=4096\n",
        "    enable_padding=True\n",
        "\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "    if shrink > 1:\n",
        "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "        quad /= shrink\n",
        "        qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "        img = img.crop(crop)\n",
        "        quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "    if enable_padding and max(pad) > border - 4:\n",
        "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "        h, w, _ = img.shape\n",
        "        y, x, _ = np.ogrid[:h, :w, :1]\n",
        "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "        blur = qsize * 0.02\n",
        "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "        img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "        quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "    if output_size < transform_size:\n",
        "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "    # Save aligned image.\n",
        "    return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbPdBBobg-X3"
      },
      "source": [
        "#@title 🖼 Align the portrait { display-mode: \"form\" }\n",
        "\n",
        "#@markdown 💬 Name of the previously upload image\n",
        "input_image = \"portrait.jpg\" #@param {type:\"string\"}\n",
        "#@markdown 💬 Name of the cropped and aligned image\n",
        "output_image = \"portrait_aligned.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "aligned = align_face(os.path.join(\"../images/\", input_image))\n",
        "aligned.save(os.path.join(\"../images/\", output_image))\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(os.path.join(\"../images/\", output_image), width=256, height=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HklBPrmRMsDG"
      },
      "source": [
        "#@title ▶ Define some functions needed to find an image in the latent space { display-mode: \"form\" }\n",
        "\n",
        "def project(\n",
        "    G,\n",
        "    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
        "    *,\n",
        "    num_steps                  = 1000,\n",
        "    w_avg_samples              = 10000,\n",
        "    initial_learning_rate      = 0.1,\n",
        "    initial_noise_factor       = 0.05,\n",
        "    lr_rampdown_length         = 0.25,\n",
        "    lr_rampup_length           = 0.05,\n",
        "    noise_ramp_length          = 0.75,\n",
        "    regularize_noise_weight    = 1e5,\n",
        "    verbose                    = False,\n",
        "    device: torch.device\n",
        "):\n",
        "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
        "\n",
        "    def logprint(*args):\n",
        "        if verbose:\n",
        "            print(*args)\n",
        "\n",
        "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n",
        "\n",
        "    # Compute w stats.\n",
        "    logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
        "    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
        "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
        "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n",
        "    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
        "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
        "\n",
        "    # Setup noise inputs.\n",
        "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
        "\n",
        "    # Load VGG16 feature detector.\n",
        "    url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
        "    with dnnlib.util.open_url(url) as f:\n",
        "        vgg16 = torch.jit.load(f).eval().to(device)\n",
        "\n",
        "    # Features for target image.\n",
        "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
        "    if target_images.shape[2] > 256:\n",
        "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
        "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
        "\n",
        "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
        "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
        "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "\n",
        "    # Init noise.\n",
        "    for buf in noise_bufs.values():\n",
        "        buf[:] = torch.randn_like(buf)\n",
        "        buf.requires_grad = True\n",
        "\n",
        "    pimages = []\n",
        "\n",
        "    pbar = tqdm(range(num_steps), position=0)\n",
        "    for step in pbar:\n",
        "        # Learning rate schedule.\n",
        "        t = step / num_steps\n",
        "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "        lr = initial_learning_rate * lr_ramp\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Synth images from opt_w.\n",
        "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
        "        synth_images = G.synthesis(ws, noise_mode='const')\n",
        "\n",
        "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
        "        synth_images = (synth_images + 1) * (255/2)\n",
        "        if synth_images.shape[2] > 256:\n",
        "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
        "\n",
        "        # Features for synth images.\n",
        "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
        "        dist = (target_features - synth_features).square().sum()\n",
        "\n",
        "        # Noise regularization.\n",
        "        reg_loss = 0.0\n",
        "        for v in noise_bufs.values():\n",
        "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
        "            while True:\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
        "                if noise.shape[2] <= 8:\n",
        "                    break\n",
        "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "        loss = dist + reg_loss * regularize_noise_weight\n",
        "\n",
        "        # Step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # logprint(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
        "        pbar.set_description(f'dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
        "\n",
        "        # Save projected W for each optimization step.\n",
        "        w_out[step] = w_opt.detach()[0]\n",
        "        if step % 25 == 0:\n",
        "          synth_image = G.synthesis(w_out[step].repeat([G.mapping.num_ws, 1]).unsqueeze(0), noise_mode='const')\n",
        "          im = torch_to_image(synth_image)\n",
        "          pimages.insert(0, im.resize((128, 128)))\n",
        "\n",
        "          clear_output()\n",
        "          media.show_images(pimages, height=128)\n",
        "\n",
        "        # Normalize noise.\n",
        "        with torch.no_grad():\n",
        "            for buf in noise_bufs.values():\n",
        "                buf -= buf.mean()\n",
        "                buf *= buf.square().mean().rsqrt()\n",
        "\n",
        "    return w_out.repeat([1, G.mapping.num_ws, 1])\n",
        "\n",
        "def run_projection(image_file, seed, num_steps):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Load target image.\n",
        "    target_pil = PIL.Image.open(image_file).convert('RGB')\n",
        "    w, h = target_pil.size\n",
        "    s = min(w, h)\n",
        "    target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
        "    target_pil = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)\n",
        "    target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
        "\n",
        "    # Optimize projection.\n",
        "    projected_w_steps = project(\n",
        "        G,\n",
        "        target=torch.tensor(target_uint8.transpose([2, 0, 1]), device=device), # pylint: disable=not-callable\n",
        "        num_steps=num_steps,\n",
        "        device=device,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Save final projected frame and W vector.\n",
        "    projected_w = projected_w_steps[-1]\n",
        "    synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "    img = torch_to_image(synth_image)\n",
        "    display(img)\n",
        "\n",
        "    np.savez(image_file + \".npz\", w=projected_w.unsqueeze(0).cpu().numpy())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfXga0TfVmSl",
        "cellView": "form"
      },
      "source": [
        "#@title 🖼 Find the uploaded image in the latent space\n",
        "\n",
        "#@markdown Now we will try to find the point in the latent space that generates the image that is more similar to the portrait.\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "#@markdown 💬 Name of the cropped and aligned image\n",
        "input_image = \"portrait_aligned.jpg\" #@param {type:\"string\"}\n",
        "#@markdown 💬 Number of steps of the search. More steps is better, but it takes longer.\n",
        "steps = 1000#@param {type:\"integer\"}\n",
        "\n",
        "run_projection(os.path.join(\"../images/\", input_image), 27, steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzoMqgfIbJR5"
      },
      "source": [
        "#@title 🖼 Modify yourself inside the latent space { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "#@markdown Now we can modify our portrait in the latent space as we did before with the seeds.\n",
        "\n",
        "#@markdown 💬 Name of the cropped and aligned image\n",
        "input_position = \"portrait_aligned.jpg.npz\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown 💬 Index of the direction and amount of change\n",
        "factor_index = 1#@param {type:\"integer\"}\n",
        "factor_degree = 3#@param {type:\"slider\", min:0, max:10, step:0.25}\n",
        "#@markdown 💬 Size of the output images\n",
        "image_size = \"512\"#@param [128, 256, 512, 1024]\n",
        "\n",
        "factor_index = [factor_index]\n",
        "\n",
        "portrait_w = np.load(os.path.join(\"../images/\", input_position))\n",
        "\n",
        "factorize(torch.from_numpy(np.array([portrait_w['w']])).to(device), 'w', factor_index, factor_degree, image_size=int(image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRkACBXXPlWp"
      },
      "source": [
        "# Finding someone inside latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GoE_FlUw78B"
      },
      "source": [
        "#@title ▶ Define the functions necessary to find inside the latent space by a description { display-mode: \"form\" }\n",
        "\n",
        "def clip_approach(\n",
        "    G,\n",
        "    *,\n",
        "    num_steps                  = 100,\n",
        "    w_avg_samples              = 10000,\n",
        "    initial_learning_rate      = 0.02,\n",
        "    initial_noise_factor       = 0.02,\n",
        "    noise_floor                = 0.02,\n",
        "    psi                        = 0.8,\n",
        "    noise_ramp_length          = 1.0, # was 0.75\n",
        "    regularize_noise_weight    = 10000, # was 1e5\n",
        "    seed                       = 69097,\n",
        "    autoseed                   = True,\n",
        "    autoseed_samples           = 128,\n",
        "    noise_opt                  = True,\n",
        "    ws                         = None,\n",
        "    text                       = 'a computer generated image',\n",
        "    device: torch.device\n",
        "):\n",
        "\n",
        "    '''\n",
        "    local_args = dict(locals())\n",
        "    params = []\n",
        "    for x in local_args:\n",
        "        if x != 'G' and x != 'device':\n",
        "            print(x,':',local_args[x])\n",
        "            params.append({x:local_args[x]})\n",
        "    print(json.dumps(params))\n",
        "    '''\n",
        "\n",
        "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device)\n",
        "    lr = initial_learning_rate\n",
        "\n",
        "    # Load the perceptor\n",
        "    print('Loading perceptor for text:', text)\n",
        "    perceptor, preprocess = clip.load('ViT-B/32', jit=True)\n",
        "    perceptor = perceptor.eval()\n",
        "    tx = clip.tokenize(text)\n",
        "    whispers = perceptor.encode_text(tx.cuda()).detach().clone()\n",
        "\n",
        "    # autoseed\n",
        "    if autoseed:\n",
        "      seed = clip_find_best_seed(seed, perceptor, whispers, autoseed_samples, psi)\n",
        "\n",
        "    # derive W from seed\n",
        "    if ws is None:\n",
        "        print('Generating w for seed %i' % seed )\n",
        "        z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n",
        "        w_samples = G.mapping(z,  None, truncation_psi=psi)\n",
        "        w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)\n",
        "        w_avg = np.mean(w_samples, axis=0, keepdims=True)\n",
        "    else:\n",
        "        w_samples = torch.tensor(ws, device=device)\n",
        "        w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)\n",
        "        w_avg = np.mean(w_samples, axis=0, keepdims=True)\n",
        "    #w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
        "    w_std = 2 # ~9.9 for portraits network. should compute if using median median\n",
        "\n",
        "    # Setup noise inputs.\n",
        "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
        "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
        "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
        "\n",
        "    if noise_opt:\n",
        "        optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "        print('optimizer: w + noise')\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam([w_opt] , betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "        print('optimizer: w')\n",
        "\n",
        "    # Init noise.\n",
        "    for buf in noise_bufs.values():\n",
        "        buf[:] = torch.randn_like(buf)\n",
        "        buf.requires_grad = True\n",
        "\n",
        "    pimages = []\n",
        "\n",
        "    # Descend\n",
        "    pbar = tqdm(range(num_steps))\n",
        "    for step in pbar:\n",
        "        # noise schedule\n",
        "        t = step / num_steps\n",
        "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "\n",
        "        # floor\n",
        "        if w_noise_scale < noise_floor:\n",
        "            w_noise_scale = noise_floor\n",
        "\n",
        "        # lr schedule is disabled\n",
        "        '''\n",
        "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "        lr = initial_learning_rate * lr_ramp\n",
        "        '''\n",
        "\n",
        "        ''' for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        '''\n",
        "\n",
        "        # do G.synthesis\n",
        "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
        "        synth_images = G.synthesis(ws, noise_mode='const')\n",
        "\n",
        "        #save1\n",
        "        '''\n",
        "        synth_images_save = (synth_images + 1) * (255/2)\n",
        "        synth_images_save = synth_images_save.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "        PIL.Image.fromarray(synth_images_save, 'RGB').save('project/test1.png')\n",
        "        '''\n",
        "\n",
        "        nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        into = synth_images\n",
        "        into = nom(into) # normalize copied from CLIP preprocess. doesn't seem to affect tho\n",
        "\n",
        "        # scale to CLIP input size\n",
        "        into = torch.nn.functional.interpolate(synth_images, (224,224), mode='bilinear', align_corners=True)\n",
        "\n",
        "        # CLIP expects [1, 3, 224, 224], so we should be fine\n",
        "        glimmers = perceptor.encode_image(into)\n",
        "        proximity =  -30 * torch.cosine_similarity(whispers, glimmers, dim = -1).mean() # Dunno why 30 works lol\n",
        "\n",
        "        # noise reg, from og projector\n",
        "        reg_loss = 0.0\n",
        "        for v in noise_bufs.values():\n",
        "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
        "            while True:\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
        "                if noise.shape[2] <= 8:\n",
        "                    break\n",
        "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "\n",
        "        if noise_opt:\n",
        "            loss = proximity + reg_loss * regularize_noise_weight\n",
        "        else:\n",
        "            loss = proximity\n",
        "\n",
        "        # Step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print(f'step {step+1:>4d}/{num_steps}:  loss {float(loss):<5.2f} ','lr',\n",
        "        #       lr, f'noise scale: {float(w_noise_scale):<5.6f}',f'proximity: {float(proximity / (-30)):<5.6f}')\n",
        "        pbar.set_description(f'loss {float(loss):<5.2f} | proximity: {float(proximity / (-30)):<5.6f}')\n",
        "\n",
        "        # Save projected W for each optimization step.\n",
        "        w_out[step] = w_opt.detach()[0]\n",
        "        if step % 25 == 0:\n",
        "          synth_image = G.synthesis(w_out[step].repeat([G.mapping.num_ws, 1]).unsqueeze(0), noise_mode='const')\n",
        "          im = torch_to_image(synth_image)\n",
        "          pimages.insert(0, im.resize((128, 128)))\n",
        "\n",
        "          clear_output()\n",
        "          media.show_images(pimages, height=128)\n",
        "\n",
        "\n",
        "        # Normalize noise.\n",
        "        with torch.no_grad():\n",
        "            for buf in noise_bufs.values():\n",
        "                buf -= buf.mean()\n",
        "                buf *= buf.square().mean().rsqrt()\n",
        "\n",
        "    return w_out.repeat([1, G.mapping.num_ws, 1])\n",
        "\n",
        "def clip_search(text, seed, autoseed, num_steps, truncation_psi=0.8):\n",
        "  # dummy\n",
        "  ws = None\n",
        "  outdir = '../images'\n",
        "  save_video = False\n",
        "\n",
        "  psi = 0.8\n",
        "  initial_learning_rate = 0.02\n",
        "  initial_noise_factor = 0.04 # 0.02 originally\n",
        "  noise_floor = 0.02\n",
        "  # If noise_opt is true then we're optimizing w and noise vars (default behaviour)\n",
        "  noise_opt = True\n",
        "\n",
        "  # approach\n",
        "  projected_w_steps = clip_approach(\n",
        "      G,\n",
        "      num_steps=num_steps,\n",
        "      device=device,\n",
        "      initial_learning_rate = initial_learning_rate,\n",
        "      psi = truncation_psi,\n",
        "      seed = seed,\n",
        "      initial_noise_factor = initial_noise_factor,\n",
        "      noise_floor = noise_floor,\n",
        "      text = text,\n",
        "      autoseed = autoseed,\n",
        "      ws = ws,\n",
        "      noise_opt = noise_opt\n",
        "  )\n",
        "\n",
        "  # save video\n",
        "  os.makedirs(outdir, exist_ok=True)\n",
        "  if save_video:\n",
        "      video = imageio.get_writer(f'{outdir}/out.mp4', mode='I', fps=10, codec='libx264', bitrate='16M')\n",
        "      print (f'Saving optimization progress video \"{outdir}/out.mp4\"')\n",
        "      for projected_w in projected_w_steps:\n",
        "          synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "          synth_image = (synth_image + 1) * (255/2)\n",
        "          synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "          video.append_data(np.concatenate([synth_image], axis=1))\n",
        "      video.close()\n",
        "\n",
        "  '''\n",
        "  # save ws\n",
        "  if save_ws:\n",
        "      print ('Saving optimization progress ws')\n",
        "      step = 0\n",
        "      for projected_w in projected_w_steps:\n",
        "          np.savez(f'{outdir}/w-{hashname}-{step}.npz', w=projected_w.unsqueeze(0).cpu().numpy())\n",
        "          step+=1\n",
        "  '''\n",
        "\n",
        "  # save the result and the final w\n",
        "  # print ('Saving finals')\n",
        "  projected_w = projected_w_steps[-1]\n",
        "  synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "  synth_image = (synth_image + 1) * (255/2)\n",
        "  synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "  im = PIL.Image.fromarray(synth_image, 'RGB')\n",
        "  display(im)\n",
        "\n",
        "  np.savez(f'{outdir}/{text.replace(\" \", \"_\")}.npz', w=projected_w.unsqueeze(0).cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def clip_find_best_seed(seed, perceptor, whispers, autoseed_samples, psi):\n",
        "  print(f'Guessing the best seed using {autoseed_samples} samples')\n",
        "\n",
        "  random.seed(seed)\n",
        "\n",
        "  pod = np.full((autoseed_samples),0)\n",
        "  for i in range(autoseed_samples):\n",
        "      seed = randint(0,500000)\n",
        "      pod[i] = seed\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  series = []\n",
        "  for i in range(autoseed_samples):\n",
        "      snap = G(torch.from_numpy(np.random.RandomState(pod[i]).randn(1,G.z_dim)).to(device), None, truncation_psi=psi, noise_mode='const')\n",
        "      snap = torch.nn.functional.interpolate(snap, (224,224), mode='bilinear', align_corners=True)\n",
        "      # fitness = int( torch.cosine_similarity(whispers, perceptor.encode_image(snap), dim = -1).cpu().detach().numpy() * 1000)\n",
        "      # fitness = int( spherical_dist_loss(whispers, perceptor.encode_image(snap) ).cpu().detach().numpy() * 1000)\n",
        "      fitness = int( spherical_dist_loss(whispers, perceptor.encode_image(normalize(snap.add(1).div(2))) ).cpu().detach().numpy() * 1000)\n",
        "\n",
        "      series.append( (pod[i], fitness ))\n",
        "\n",
        "  series = sorted(series,key=lambda x:(x[1]))\n",
        "\n",
        "  # for i in range(4):\n",
        "  #   print(i, series[i][0], series[i][1])\n",
        "  #   z = np.random.RandomState(series[i][0]).randn(1, G.z_dim)\n",
        "  #   im = generate_image(z, truncation_psi)\n",
        "  #   display(im.resize((256, 256)))\n",
        "\n",
        "  # print (f'Top guess {series[0][0]}')\n",
        "  seed = series[0][0]\n",
        "  return seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk4jzTSAx24k"
      },
      "source": [
        "#@title 🖼 Search inside the latent space by a description { display-mode: \"form\" }\n",
        "\n",
        "#@markdown 💬 The description of the image to search for.\n",
        "text = \"a portrait of john malkovich\" #@param {type:\"string\"}\n",
        "#@markdown 💬 Starting seed. It is important to chose a starting seed similar to the description to make it faster and with more probabilities of working.\n",
        "seed =  1#@param {type:\"integer\"}\n",
        "#@markdown 💬 Try to automatically find a good starting seed (sometimes works, sometimes not)\n",
        "autoseed =  False#@param {type:\"boolean\"}\n",
        "#@markdown 💬 Number of steps of the search. More steps is better, but it takes longer.\n",
        "num_steps = 100#@param {type:\"integer\"}\n",
        "\n",
        "clip_search(text, seed, autoseed, num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX08_ZPZkL5U"
      },
      "source": [
        "# Download\n",
        "\n",
        "Download the found positions inside the latent space (.npz files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y0hUw3fkUp7"
      },
      "source": [
        "!zip -qr -0 /content/images.zip /content/images\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/images.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0pv_AU_OK0r"
      },
      "source": [
        "#Related Collab Notebooks and resources\n",
        "\n",
        "Search inside faces and other latent spaces: [StyleGAN3+CLIP Online](https://replicate.com/ouhenio/stylegan3-clip) /\n",
        "[StyleGAN3+CLIP Notebook](https://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2BCLIP.ipynb)\n",
        "\n",
        "Find a face inside the latent space and modify with text descriptions:  [StyleClip online](https://replicate.com/orpatashnik/styleclip) / [Styleclip notebooks at Github](https://github.com/orpatashnik/StyleCLIP)\n",
        "\n",
        "Search inside a more general latent space: [VQGAN+CLIP online](https://huggingface.co/spaces/multimodalart/vqgan) / [VQGAN+CLIP notebook](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP%28Updated%29.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTHr_I7cOIH_"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Taller Estampa https://tallerestampa.com / https://github.com/estampa\n",
        "\n",
        "### Based on\n",
        "Based on notebooks of [pbaylies fork](https://github.com/pbaylies/stylegan2-ada-pytorch) of [dvschultz Stylegan2-ADA Pytorch fork](https://github.com/dvschultz/stylegan2-ada-pytorch)"
      ]
    }
  ]
}