{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IAAC - 1 - Latent Space.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfke20gNnvde"
      },
      "source": [
        "# Network initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9EZXi4J1A6r9"
      },
      "source": [
        "#@title â–¶ Download the neural networks from github and the training for generation faces\n",
        "\n",
        "#@markdown We will use Styelgan2-ADA to generate faces and at the end we will use CLIP to search inside the latent space\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/pbaylies/stylegan2-ada-pytorch.git\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!cp -R CLIP/clip/ stylegan2-ada-pytorch\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "%mkdir weights\n",
        "%cd weights\n",
        "!wget -nc https://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MSWo-82yBl88"
      },
      "source": [
        "#@title â–¶ Install the required libraries\n",
        "\n",
        "#@markdown It may take a while (3 minutes)\n",
        "\n",
        "!apt install -y -q ninja-build\n",
        "!pip -q install mediapy opensimplex ftfy ninja\n",
        "\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu117\"\n",
        "\n",
        "!pip -q install torch==1.13.1{torch_version_suffix} torchvision==0.14.1{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXuO0sLpnq4L",
        "cellView": "form"
      },
      "source": [
        "#@title â–¶ Initialize the network and define some general functions\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "# General imports\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import PIL.Image\n",
        "import random\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import os.path\n",
        "import copy\n",
        "\n",
        "# Notebook imports\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "import mediapy as media\n",
        "\n",
        "# Neural network imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Imports from Network\n",
        "import dnnlib\n",
        "import legacy\n",
        "from generate import seeds_to_zs, line_interpolate\n",
        "import clip\n",
        "import apply_factor\n",
        "\n",
        "\n",
        "custom = False\n",
        "network_pkl = 'weights/stylegan2-ffhq-config-f.pkl'\n",
        "\n",
        "G_kwargs = dnnlib.EasyDict()\n",
        "#G_kwargs.size = size\n",
        "#G_kwargs.scale_type = scale_type\n",
        "\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "    # G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "    G = legacy.load_network_pkl(f, custom=custom, **G_kwargs)['G_ema'].to(device) # type: ignore\n",
        "\n",
        "label = torch.zeros([1, G.c_dim], device=device)\n",
        "\n",
        "def generate_image(z, truncation_psi, to_pil=True):\n",
        "  zt = torch.from_numpy(z).to(device)\n",
        "  img = G(zt, label, truncation_psi=truncation_psi, noise_mode='const')\n",
        "  return torch_to_image(img, to_pil)\n",
        "\n",
        "def torch_to_image(tensor, to_pil=True):\n",
        "  img = (tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "  if to_pil:\n",
        "    return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')\n",
        "  else:\n",
        "    return img[0].cpu().numpy()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBWAISb1Mv_g"
      },
      "source": [
        "# Generating images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQhp7vKvsi1i"
      },
      "source": [
        "#@title ðŸ–¼ Generate an image { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "#@markdown ðŸ’¬ A seed is a shortcut to a point in the latent space so there's no need to write a value for each of the 512 dimensions.\n",
        "seed = 1#@param {type:\"integer\"}\n",
        "#@markdown ðŸ’¬ Truncation PSI is a parameter of the network that defines how \"strange\" are the images that it generates. \"Strange\" meaning away from to the \"average\" \"intermediate\".\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "z = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "im = generate_image(z, truncation_psi)\n",
        "display(im.resize((512, 512)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147KAkqvkt1y"
      },
      "source": [
        "#@title ðŸ–¼ Generate multiple images { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ðŸ’¬ Select the number of images and the start seed. The images that will be generated will be for the seeds start_seed to start_seed + num_seeds.\n",
        "start_seed = 1#@param {type:\"integer\"}\n",
        "num_seeds = 64#@param {type:\"integer\"}\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown ðŸ’¬ Display size for each image.\n",
        "image_size = 128#@param [32, 64, 128, 256, 512]\n",
        "\n",
        "image_size = int(image_size)\n",
        "\n",
        "seeds = [seed for seed in range(start_seed, start_seed+num_seeds+1)]\n",
        "seeds_text = [str(seed) for seed in seeds]\n",
        "\n",
        "points = seeds_to_zs(G, seeds)\n",
        "\n",
        "images = []\n",
        "for point in tqdm(points, leave=False):\n",
        "  im = generate_image(point, truncation_psi, False)\n",
        "  images.append(cv2.resize(im, dsize=(image_size, image_size), interpolation=cv2.INTER_CUBIC));\n",
        "\n",
        "columns = int(1536/image_size)\n",
        "media.show_images(images, border=True, height=image_size, columns=columns, titles=seeds_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLvHLEU5M3OH"
      },
      "source": [
        "# Navigating the latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA6y2FIZtlas"
      },
      "source": [
        "#@title ðŸ–¼ Generate images from points close to one seed { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "#@markdown By generating images to points close to one seed we see that they are similar. Like generating the image at [1,1] and the imatges at [1,1.01], [1,1.02], ...\n",
        "\n",
        "def generate_variations(seed, num_variations):\n",
        "  # print(G.z_dim)\n",
        "  z = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "  # print(z)\n",
        "  im = generate_image(z, truncation_psi)\n",
        "\n",
        "  images = [im]\n",
        "\n",
        "  for i in range(0,num_variations):\n",
        "    z[0,i] = 1\n",
        "    im = generate_image(z, truncation_psi)\n",
        "    images.append(im)\n",
        "\n",
        "  media.show_images(images, border=True, height=128)\n",
        "\n",
        "seed = 1#@param {type:\"integer\"}\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "generate_variations(seed, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1SM7baF2kQY"
      },
      "source": [
        "#@title ðŸ–¼ Generate an interpolation video between two seeds { display-mode: \"form\" }\n",
        "\n",
        "#@markdown We can also generate images for the points that are between two seeds, and the convert it to a video.\n",
        "\n",
        "def generate_interpolation(seed_1, seed_2, frames, easing, video_size):\n",
        "  points = seeds_to_zs(G,[seed_1,seed_2])\n",
        "  points = line_interpolate(points,frames,easing)\n",
        "\n",
        "  images = []\n",
        "  for idx, point in enumerate(tqdm(points, leave=False)):\n",
        "    im = generate_image(point, truncation_psi, False)\n",
        "    images.append(cv2.resize(im, dsize=(video_size, video_size), interpolation=cv2.INTER_CUBIC));\n",
        "\n",
        "  media.show_video(images, fps=8)\n",
        "\n",
        "#@markdown ðŸ’¬ Starting and ending seed\n",
        "seed1 = 1#@param {type:\"integer\"}\n",
        "seed2 = 2#@param {type:\"integer\"}\n",
        "truncation_psi = 1#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown ðŸ’¬ Number of frames of the video: more frames takes longer and the transition is slower\n",
        "frames = 100#@param {type:\"integer\"}\n",
        "#@markdown ðŸ’¬ Dimensions of the generated video\n",
        "video_size = \"512\"#@param [256, 512, 1024]\n",
        "\n",
        "#@markdown â˜ The video can be downloaded right clicking in it\n",
        "\n",
        "generate_interpolation(seed1,seed2,frames, 'linear', int(video_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBJlVQVv7RTn"
      },
      "source": [
        "#@title ðŸ–¼ Generate an interpolation matrix between four seeds { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Visualize the interpolation four-side\n",
        "\n",
        "def generate_matrix(seed_1, seed_2, seed_3, seed_4, steps, easing, truncation_psi=0.8, image_size=64):\n",
        "  points = seeds_to_zs(G,[seed_1,seed_2,seed_3,seed_4])\n",
        "  a_to_b = line_interpolate([points[0], points[1]], steps, easing)\n",
        "  c_to_d = line_interpolate([points[2], points[3]], steps, easing)\n",
        "\n",
        "  images = []\n",
        "\n",
        "  for step in range(0, steps):\n",
        "    row_points = line_interpolate([a_to_b[step], c_to_d[step]], steps, easing)\n",
        "    for point in row_points:\n",
        "      im = generate_image(point, truncation_psi, False)\n",
        "      images.append(cv2.resize(im, dsize=(256, 256), interpolation=cv2.INTER_CUBIC));\n",
        "\n",
        "  media.show_images(images, border=True, height=64, columns=steps)\n",
        "\n",
        "#@markdown ðŸ’¬ Seeds at the corners\n",
        "seed1 = 1#@param {type:\"integer\"}\n",
        "seed2 = 2#@param {type:\"integer\"}\n",
        "seed3 = 3#@param {type:\"integer\"}\n",
        "seed4 = 4#@param {type:\"integer\"}\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "#@markdown ðŸ’¬ Number and size of the interpolation images\n",
        "steps = 8#@param {type:\"integer\"}\n",
        "image_size = \"64\"#@param [64, 128, 256]\n",
        "\n",
        "generate_matrix(seed1, seed2, seed3, seed4, steps, 'linear', truncation_psi, int(image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usim2M07NFmU"
      },
      "source": [
        "# Trying to make sense of the directions in this space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kykdtt9BqfM"
      },
      "source": [
        "#@title â–¶ Extracting the directions and defining necessary functions { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Using a command from the network a new file is created that contains some directions of the latents space that we will later use.\n",
        "\n",
        "#@markdown Also, some functions to work with the directions are defined.\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "! python closed_form_factorization.py --ckpt {network_pkl} --out {network_pkl}.pt\n",
        "\n",
        "def generate_factorized_images(z_w, label, truncation_psi, noise_mode, direction, space):\n",
        "    if(space == 'w'):\n",
        "        img1 = G.synthesis(z_w, noise_mode=noise_mode, force_fp32=True)\n",
        "        img2 = G.synthesis(z_w + direction, noise_mode=noise_mode, force_fp32=True)\n",
        "        img3 = G.synthesis(z_w - direction, noise_mode=noise_mode, force_fp32=True)\n",
        "    else:\n",
        "        img1 = G(z_w, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "        img2 = G(z_w + direction, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "        img3 = G(z_w - direction, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "\n",
        "    return [torch_to_image(img3), torch_to_image(img1), torch_to_image(img2)]\n",
        "\n",
        "\n",
        "def factorize(latents, space, index, degree, truncation_psi=0.8, image_size=128):\n",
        "\n",
        "  device = torch.device('cuda')\n",
        "  eigvec = torch.load(network_pkl + \".pt\")[\"eigvec\"].to(device)\n",
        "\n",
        "  custom = False\n",
        "\n",
        "  label = torch.zeros([1, G.c_dim], device=device) # assume no class label\n",
        "  noise_mode = \"const\" # default\n",
        "\n",
        "  index_list_of_eigenvalues = []\n",
        "\n",
        "  image_grid_eigvec = [[],[],[]]\n",
        "\n",
        "  for l in latents:\n",
        "      if len(index) ==  1 and index[0] == -1: # use all eigenvalues\n",
        "          index_list_of_eigenvalues = [*range(len(eigvec))]\n",
        "      else: # use certain indexes as eigenvalues\n",
        "          index_list_of_eigenvalues = index\n",
        "\n",
        "      for j in index_list_of_eigenvalues:\n",
        "          current_eigvec = eigvec[:, j].unsqueeze(0)\n",
        "          direction = degree * current_eigvec\n",
        "          image_group = generate_factorized_images(l, label, truncation_psi, noise_mode, direction, space)\n",
        "\n",
        "          image_grid_eigvec[0].append(image_group[0])\n",
        "          image_grid_eigvec[1].append(image_group[1])\n",
        "          image_grid_eigvec[2].append(image_group[2])\n",
        "\n",
        "          # media.show_images(image_group, border=False, height=128)\n",
        "\n",
        "  columns = len(latents)\n",
        "  if len(latents) == 1:\n",
        "    columns = 3\n",
        "  media.show_images(image_grid_eigvec[0] + image_grid_eigvec[1] + image_grid_eigvec[2], border=False, height=image_size, columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqimzzU_479W"
      },
      "source": [
        "#@title ðŸ–¼ Modify some seeds in one of the found directions { display-mode: \"form\" }\n",
        "\n",
        "#@markdown In the previous step the tool defined a list of semantic directions, now we can modify the seeds in those directions. We don't know what each direction means, so we have to try for each one.\n",
        "\n",
        "#@markdown The tool will generate the seed image and the seed image modified in the positive and negative direction for 8 seeds.\n",
        "\n",
        "#@markdown ðŸ’¬ Index of the direction\n",
        "factor_index = 1#@param {type:\"integer\"}\n",
        "#@markdown ðŸ’¬ Amount of change\n",
        "factor_degree = 2.5#@param {type:\"slider\", min:0, max:10, step:0.25}\n",
        "\n",
        "#@markdown &nbsp;\n",
        "start_seed = 1#@param {type:\"integer\"}\n",
        "truncation_psi = 0.8#@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "image_size = \"128\"#@param [64, 128, 256, 512]\n",
        "\n",
        "factor_seeds = [start_seed + i for i in range(8)]\n",
        "factor_index = [factor_index]\n",
        "\n",
        "latents_z = []\n",
        "for seed in factor_seeds:\n",
        "  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n",
        "  latents_z.append(z)\n",
        "\n",
        "latents_w = apply_factor.zs_to_ws(G,torch.device('cuda'),label,truncation_psi,latents_z)\n",
        "\n",
        "factorize(latents_w, 'w', factor_index, factor_degree, truncation_psi, int(image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOcbRyTMNOi4"
      },
      "source": [
        "# Finding you inside the latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQaOvW2gMNzm",
        "cellView": "form"
      },
      "source": [
        "#@title â¬† Upload a picture\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!mkdir /content/images\n",
        "%cd /content/images\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('Uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "%cd /content/stylegan2-ada-pytorch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFTWhBSXm-rm"
      },
      "source": [
        "#@title â–¶ Define some functions to crop and align the protrait { display-mode: \"form\" }\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "!wget -nc http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bunzip2 --keep shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "import dlib\n",
        "\n",
        "predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "def get_landmark(filepath):\n",
        "    \"\"\"get landmark with dlib\n",
        "    :return: np.array shape=(68, 2)\n",
        "    \"\"\"\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    img = dlib.load_rgb_image(filepath)\n",
        "    dets = detector(img, 1)\n",
        "\n",
        "    print(\"Number of faces detected: {}\".format(len(dets)))\n",
        "    for k, d in enumerate(dets):\n",
        "        print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n",
        "            k, d.left(), d.top(), d.right(), d.bottom()))\n",
        "        # Get the landmarks/parts for the face in box d.\n",
        "        shape = predictor(img, d)\n",
        "        print(\"Part 0: {}, Part 1: {} ...\".format(shape.part(0), shape.part(1)))\n",
        "\n",
        "\n",
        "    t = list(shape.parts())\n",
        "    a = []\n",
        "    for tt in t:\n",
        "        a.append([tt.x, tt.y])\n",
        "    lm = np.array(a)\n",
        "    # lm is a shape=(68,2) np.array\n",
        "    return lm\n",
        "\n",
        "\n",
        "def align_face(filepath):\n",
        "    \"\"\"\n",
        "    :param filepath: str\n",
        "    :return: PIL Image\n",
        "    \"\"\"\n",
        "\n",
        "    lm = get_landmark(filepath)\n",
        "\n",
        "    lm_chin          = lm[0  : 17]  # left-right\n",
        "    lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "    lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "    lm_nose          = lm[27 : 31]  # top-down\n",
        "    lm_nostrils      = lm[31 : 36]  # top-down\n",
        "    lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "    lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "    lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "    lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "    # Calculate auxiliary vectors.\n",
        "    eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "    eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "    eye_avg      = (eye_left + eye_right) * 0.5\n",
        "    eye_to_eye   = eye_right - eye_left\n",
        "    mouth_left   = lm_mouth_outer[0]\n",
        "    mouth_right  = lm_mouth_outer[6]\n",
        "    mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "    eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_avg + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "\n",
        "    # read image\n",
        "    img = PIL.Image.open(filepath)\n",
        "\n",
        "    output_size=1024\n",
        "    transform_size=4096\n",
        "    enable_padding=True\n",
        "\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "    if shrink > 1:\n",
        "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "        quad /= shrink\n",
        "        qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "        img = img.crop(crop)\n",
        "        quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "    if enable_padding and max(pad) > border - 4:\n",
        "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "        h, w, _ = img.shape\n",
        "        y, x, _ = np.ogrid[:h, :w, :1]\n",
        "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "        blur = qsize * 0.02\n",
        "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "        img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "        quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "    if output_size < transform_size:\n",
        "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "    # Save aligned image.\n",
        "    return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbPdBBobg-X3"
      },
      "source": [
        "#@title ðŸ–¼ Align the portrait { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ðŸ’¬ Name of the previously upload image\n",
        "input_image = \"portrait.jpg\" #@param {type:\"string\"}\n",
        "#@markdown ðŸ’¬ Name of the cropped and aligned image\n",
        "output_image = \"portrait_aligned.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "aligned = align_face(os.path.join(\"../images/\", input_image))\n",
        "aligned.save(os.path.join(\"../images/\", output_image))\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(os.path.join(\"../images/\", output_image), width=256, height=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HklBPrmRMsDG"
      },
      "source": [
        "#@title â–¶ Define some functions needed to find an image in the latent space { display-mode: \"form\" }\n",
        "\n",
        "def project(\n",
        "    G,\n",
        "    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
        "    *,\n",
        "    num_steps                  = 1000,\n",
        "    w_avg_samples              = 10000,\n",
        "    initial_learning_rate      = 0.1,\n",
        "    initial_noise_factor       = 0.05,\n",
        "    lr_rampdown_length         = 0.25,\n",
        "    lr_rampup_length           = 0.05,\n",
        "    noise_ramp_length          = 0.75,\n",
        "    regularize_noise_weight    = 1e5,\n",
        "    verbose                    = False,\n",
        "    device: torch.device\n",
        "):\n",
        "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
        "\n",
        "    def logprint(*args):\n",
        "        if verbose:\n",
        "            print(*args)\n",
        "\n",
        "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n",
        "\n",
        "    # Compute w stats.\n",
        "    logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
        "    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
        "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
        "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n",
        "    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
        "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
        "\n",
        "    # Setup noise inputs.\n",
        "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
        "\n",
        "    # Load VGG16 feature detector.\n",
        "    url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
        "    with dnnlib.util.open_url(url) as f:\n",
        "        vgg16 = torch.jit.load(f).eval().to(device)\n",
        "\n",
        "    # Features for target image.\n",
        "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
        "    if target_images.shape[2] > 256:\n",
        "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
        "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
        "\n",
        "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
        "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
        "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "\n",
        "    # Init noise.\n",
        "    for buf in noise_bufs.values():\n",
        "        buf[:] = torch.randn_like(buf)\n",
        "        buf.requires_grad = True\n",
        "\n",
        "    pimages = []\n",
        "\n",
        "    pbar = tqdm(range(num_steps), position=0)\n",
        "    for step in pbar:\n",
        "        # Learning rate schedule.\n",
        "        t = step / num_steps\n",
        "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "        lr = initial_learning_rate * lr_ramp\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Synth images from opt_w.\n",
        "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
        "        synth_images = G.synthesis(ws, noise_mode='const')\n",
        "\n",
        "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
        "        synth_images = (synth_images + 1) * (255/2)\n",
        "        if synth_images.shape[2] > 256:\n",
        "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
        "\n",
        "        # Features for synth images.\n",
        "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
        "        dist = (target_features - synth_features).square().sum()\n",
        "\n",
        "        # Noise regularization.\n",
        "        reg_loss = 0.0\n",
        "        for v in noise_bufs.values():\n",
        "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
        "            while True:\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
        "                if noise.shape[2] <= 8:\n",
        "                    break\n",
        "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "        loss = dist + reg_loss * regularize_noise_weight\n",
        "\n",
        "        # Step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # logprint(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
        "        pbar.set_description(f'dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
        "\n",
        "        # Save projected W for each optimization step.\n",
        "        w_out[step] = w_opt.detach()[0]\n",
        "        if step % 25 == 0:\n",
        "          synth_image = G.synthesis(w_out[step].repeat([G.mapping.num_ws, 1]).unsqueeze(0), noise_mode='const')\n",
        "          im = torch_to_image(synth_image)\n",
        "          pimages.insert(0, im.resize((128, 128)))\n",
        "\n",
        "          clear_output()\n",
        "          media.show_images(pimages, height=128)\n",
        "\n",
        "        # Normalize noise.\n",
        "        with torch.no_grad():\n",
        "            for buf in noise_bufs.values():\n",
        "                buf -= buf.mean()\n",
        "                buf *= buf.square().mean().rsqrt()\n",
        "\n",
        "    return w_out.repeat([1, G.mapping.num_ws, 1])\n",
        "\n",
        "def run_projection(image_file, seed, num_steps):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Load target image.\n",
        "    target_pil = PIL.Image.open(image_file).convert('RGB')\n",
        "    w, h = target_pil.size\n",
        "    s = min(w, h)\n",
        "    target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
        "    target_pil = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)\n",
        "    target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
        "\n",
        "    # Optimize projection.\n",
        "    projected_w_steps = project(\n",
        "        G,\n",
        "        target=torch.tensor(target_uint8.transpose([2, 0, 1]), device=device), # pylint: disable=not-callable\n",
        "        num_steps=num_steps,\n",
        "        device=device,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Save final projected frame and W vector.\n",
        "    projected_w = projected_w_steps[-1]\n",
        "    synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "    img = torch_to_image(synth_image)\n",
        "    display(img)\n",
        "\n",
        "    np.savez(image_file + \".npz\", w=projected_w.unsqueeze(0).cpu().numpy())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfXga0TfVmSl",
        "cellView": "form"
      },
      "source": [
        "#@title ðŸ–¼ Find the uploaded image in the latent space\n",
        "\n",
        "#@markdown Now we will try to find the point in the latent space that generates the image that is more similar to the portrait.\n",
        "\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "#@markdown ðŸ’¬ Name of the cropped and aligned image\n",
        "input_image = \"portrait_aligned.jpg\" #@param {type:\"string\"}\n",
        "#@markdown ðŸ’¬ Number of steps of the search. More steps is better, but it takes longer.\n",
        "steps = 1000#@param {type:\"integer\"}\n",
        "\n",
        "run_projection(os.path.join(\"../images/\", input_image), 27, steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzoMqgfIbJR5"
      },
      "source": [
        "#@title ðŸ–¼ Modify yourself inside the latent space { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "#@markdown Now we can modify our portrait in the latent space as we did before with the seeds.\n",
        "\n",
        "#@markdown ðŸ’¬ Name of the cropped and aligned image\n",
        "input_position = \"portrait_aligned.jpg.npz\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ðŸ’¬ Index of the direction and amount of change\n",
        "factor_index = 1#@param {type:\"integer\"}\n",
        "factor_degree = 3#@param {type:\"slider\", min:0, max:10, step:0.25}\n",
        "#@markdown ðŸ’¬ Size of the output images\n",
        "image_size = \"512\"#@param [128, 256, 512, 1024]\n",
        "\n",
        "factor_index = [factor_index]\n",
        "\n",
        "portrait_w = np.load(os.path.join(\"../images/\", input_position))\n",
        "\n",
        "factorize(torch.from_numpy(np.array([portrait_w['w']])).to(device), 'w', factor_index, factor_degree, image_size=int(image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRkACBXXPlWp"
      },
      "source": [
        "# Finding someone inside latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GoE_FlUw78B"
      },
      "source": [
        "#@title â–¶ Define the functions necessary to find inside the latent space by a description { display-mode: \"form\" }\n",
        "\n",
        "def clip_approach(\n",
        "    G,\n",
        "    *,\n",
        "    num_steps                  = 100,\n",
        "    w_avg_samples              = 10000,\n",
        "    initial_learning_rate      = 0.02,\n",
        "    initial_noise_factor       = 0.02,\n",
        "    noise_floor                = 0.02,\n",
        "    psi                        = 0.8,\n",
        "    noise_ramp_length          = 1.0, # was 0.75\n",
        "    regularize_noise_weight    = 10000, # was 1e5\n",
        "    seed                       = 69097,\n",
        "    autoseed                   = True,\n",
        "    autoseed_samples           = 128,\n",
        "    noise_opt                  = True,\n",
        "    ws                         = None,\n",
        "    text                       = 'a computer generated image',\n",
        "    device: torch.device\n",
        "):\n",
        "\n",
        "    '''\n",
        "    local_args = dict(locals())\n",
        "    params = []\n",
        "    for x in local_args:\n",
        "        if x != 'G' and x != 'device':\n",
        "            print(x,':',local_args[x])\n",
        "            params.append({x:local_args[x]})\n",
        "    print(json.dumps(params))\n",
        "    '''\n",
        "\n",
        "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device)\n",
        "    lr = initial_learning_rate\n",
        "\n",
        "    # Load the perceptor\n",
        "    print('Loading perceptor for text:', text)\n",
        "    perceptor, preprocess = clip.load('ViT-B/32', jit=True)\n",
        "    perceptor = perceptor.eval()\n",
        "    tx = clip.tokenize(text)\n",
        "    whispers = perceptor.encode_text(tx.cuda()).detach().clone()\n",
        "\n",
        "    # autoseed\n",
        "    if autoseed:\n",
        "      seed = clip_find_best_seed(seed, perceptor, whispers, autoseed_samples, psi)\n",
        "\n",
        "    # derive W from seed\n",
        "    if ws is None:\n",
        "        print('Generating w for seed %i' % seed )\n",
        "        z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n",
        "        w_samples = G.mapping(z,  None, truncation_psi=psi)\n",
        "        w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)\n",
        "        w_avg = np.mean(w_samples, axis=0, keepdims=True)\n",
        "    else:\n",
        "        w_samples = torch.tensor(ws, device=device)\n",
        "        w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)\n",
        "        w_avg = np.mean(w_samples, axis=0, keepdims=True)\n",
        "    #w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
        "    w_std = 2 # ~9.9 for portraits network. should compute if using median median\n",
        "\n",
        "    # Setup noise inputs.\n",
        "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
        "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
        "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
        "\n",
        "    if noise_opt:\n",
        "        optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "        print('optimizer: w + noise')\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam([w_opt] , betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "        print('optimizer: w')\n",
        "\n",
        "    # Init noise.\n",
        "    for buf in noise_bufs.values():\n",
        "        buf[:] = torch.randn_like(buf)\n",
        "        buf.requires_grad = True\n",
        "\n",
        "    pimages = []\n",
        "\n",
        "    # Descend\n",
        "    pbar = tqdm(range(num_steps))\n",
        "    for step in pbar:\n",
        "        # noise schedule\n",
        "        t = step / num_steps\n",
        "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "\n",
        "        # floor\n",
        "        if w_noise_scale < noise_floor:\n",
        "            w_noise_scale = noise_floor\n",
        "\n",
        "        # lr schedule is disabled\n",
        "        '''\n",
        "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "        lr = initial_learning_rate * lr_ramp\n",
        "        '''\n",
        "\n",
        "        ''' for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        '''\n",
        "\n",
        "        # do G.synthesis\n",
        "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
        "        synth_images = G.synthesis(ws, noise_mode='const')\n",
        "\n",
        "        #save1\n",
        "        '''\n",
        "        synth_images_save = (synth_images + 1) * (255/2)\n",
        "        synth_images_save = synth_images_save.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "        PIL.Image.fromarray(synth_images_save, 'RGB').save('project/test1.png')\n",
        "        '''\n",
        "\n",
        "        nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        into = synth_images\n",
        "        into = nom(into) # normalize copied from CLIP preprocess. doesn't seem to affect tho\n",
        "\n",
        "        # scale to CLIP input size\n",
        "        into = torch.nn.functional.interpolate(synth_images, (224,224), mode='bilinear', align_corners=True)\n",
        "\n",
        "        # CLIP expects [1, 3, 224, 224], so we should be fine\n",
        "        glimmers = perceptor.encode_image(into)\n",
        "        proximity =  -30 * torch.cosine_similarity(whispers, glimmers, dim = -1).mean() # Dunno why 30 works lol\n",
        "\n",
        "        # noise reg, from og projector\n",
        "        reg_loss = 0.0\n",
        "        for v in noise_bufs.values():\n",
        "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
        "            while True:\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
        "                if noise.shape[2] <= 8:\n",
        "                    break\n",
        "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "\n",
        "        if noise_opt:\n",
        "            loss = proximity + reg_loss * regularize_noise_weight\n",
        "        else:\n",
        "            loss = proximity\n",
        "\n",
        "        # Step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print(f'step {step+1:>4d}/{num_steps}:  loss {float(loss):<5.2f} ','lr',\n",
        "        #       lr, f'noise scale: {float(w_noise_scale):<5.6f}',f'proximity: {float(proximity / (-30)):<5.6f}')\n",
        "        pbar.set_description(f'loss {float(loss):<5.2f} | proximity: {float(proximity / (-30)):<5.6f}')\n",
        "\n",
        "        # Save projected W for each optimization step.\n",
        "        w_out[step] = w_opt.detach()[0]\n",
        "        if step % 25 == 0:\n",
        "          synth_image = G.synthesis(w_out[step].repeat([G.mapping.num_ws, 1]).unsqueeze(0), noise_mode='const')\n",
        "          im = torch_to_image(synth_image)\n",
        "          pimages.insert(0, im.resize((128, 128)))\n",
        "\n",
        "          clear_output()\n",
        "          media.show_images(pimages, height=128)\n",
        "\n",
        "\n",
        "        # Normalize noise.\n",
        "        with torch.no_grad():\n",
        "            for buf in noise_bufs.values():\n",
        "                buf -= buf.mean()\n",
        "                buf *= buf.square().mean().rsqrt()\n",
        "\n",
        "    return w_out.repeat([1, G.mapping.num_ws, 1])\n",
        "\n",
        "def clip_search(text, seed, autoseed, num_steps, truncation_psi=0.8):\n",
        "  # dummy\n",
        "  ws = None\n",
        "  outdir = '../images'\n",
        "  save_video = False\n",
        "\n",
        "  psi = 0.8\n",
        "  initial_learning_rate = 0.02\n",
        "  initial_noise_factor = 0.04 # 0.02 originally\n",
        "  noise_floor = 0.02\n",
        "  # If noise_opt is true then we're optimizing w and noise vars (default behaviour)\n",
        "  noise_opt = True\n",
        "\n",
        "  # approach\n",
        "  projected_w_steps = clip_approach(\n",
        "      G,\n",
        "      num_steps=num_steps,\n",
        "      device=device,\n",
        "      initial_learning_rate = initial_learning_rate,\n",
        "      psi = truncation_psi,\n",
        "      seed = seed,\n",
        "      initial_noise_factor = initial_noise_factor,\n",
        "      noise_floor = noise_floor,\n",
        "      text = text,\n",
        "      autoseed = autoseed,\n",
        "      ws = ws,\n",
        "      noise_opt = noise_opt\n",
        "  )\n",
        "\n",
        "  # save video\n",
        "  os.makedirs(outdir, exist_ok=True)\n",
        "  if save_video:\n",
        "      video = imageio.get_writer(f'{outdir}/out.mp4', mode='I', fps=10, codec='libx264', bitrate='16M')\n",
        "      print (f'Saving optimization progress video \"{outdir}/out.mp4\"')\n",
        "      for projected_w in projected_w_steps:\n",
        "          synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "          synth_image = (synth_image + 1) * (255/2)\n",
        "          synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "          video.append_data(np.concatenate([synth_image], axis=1))\n",
        "      video.close()\n",
        "\n",
        "  '''\n",
        "  # save ws\n",
        "  if save_ws:\n",
        "      print ('Saving optimization progress ws')\n",
        "      step = 0\n",
        "      for projected_w in projected_w_steps:\n",
        "          np.savez(f'{outdir}/w-{hashname}-{step}.npz', w=projected_w.unsqueeze(0).cpu().numpy())\n",
        "          step+=1\n",
        "  '''\n",
        "\n",
        "  # save the result and the final w\n",
        "  # print ('Saving finals')\n",
        "  projected_w = projected_w_steps[-1]\n",
        "  synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "  synth_image = (synth_image + 1) * (255/2)\n",
        "  synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "  im = PIL.Image.fromarray(synth_image, 'RGB')\n",
        "  display(im)\n",
        "\n",
        "  np.savez(f'{outdir}/{text.replace(\" \", \"_\")}.npz', w=projected_w.unsqueeze(0).cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def clip_find_best_seed(seed, perceptor, whispers, autoseed_samples, psi):\n",
        "  print(f'Guessing the best seed using {autoseed_samples} samples')\n",
        "\n",
        "  random.seed(seed)\n",
        "\n",
        "  pod = np.full((autoseed_samples),0)\n",
        "  for i in range(autoseed_samples):\n",
        "      seed = randint(0,500000)\n",
        "      pod[i] = seed\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  series = []\n",
        "  for i in range(autoseed_samples):\n",
        "      snap = G(torch.from_numpy(np.random.RandomState(pod[i]).randn(1,G.z_dim)).to(device), None, truncation_psi=psi, noise_mode='const')\n",
        "      snap = torch.nn.functional.interpolate(snap, (224,224), mode='bilinear', align_corners=True)\n",
        "      # fitness = int( torch.cosine_similarity(whispers, perceptor.encode_image(snap), dim = -1).cpu().detach().numpy() * 1000)\n",
        "      # fitness = int( spherical_dist_loss(whispers, perceptor.encode_image(snap) ).cpu().detach().numpy() * 1000)\n",
        "      fitness = int( spherical_dist_loss(whispers, perceptor.encode_image(normalize(snap.add(1).div(2))) ).cpu().detach().numpy() * 1000)\n",
        "\n",
        "      series.append( (pod[i], fitness ))\n",
        "\n",
        "  series = sorted(series,key=lambda x:(x[1]))\n",
        "\n",
        "  # for i in range(4):\n",
        "  #   print(i, series[i][0], series[i][1])\n",
        "  #   z = np.random.RandomState(series[i][0]).randn(1, G.z_dim)\n",
        "  #   im = generate_image(z, truncation_psi)\n",
        "  #   display(im.resize((256, 256)))\n",
        "\n",
        "  # print (f'Top guess {series[0][0]}')\n",
        "  seed = series[0][0]\n",
        "  return seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk4jzTSAx24k"
      },
      "source": [
        "#@title ðŸ–¼ Search inside the latent space by a description { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ðŸ’¬ The description of the image to search for.\n",
        "text = \"a portrait of john malkovich\" #@param {type:\"string\"}\n",
        "#@markdown ðŸ’¬ Starting seed. It is important to chose a starting seed similar to the description to make it faster and with more probabilities of working.\n",
        "seed =  1#@param {type:\"integer\"}\n",
        "#@markdown ðŸ’¬ Try to automatically find a good starting seed (sometimes works, sometimes not)\n",
        "autoseed =  False#@param {type:\"boolean\"}\n",
        "#@markdown ðŸ’¬ Number of steps of the search. More steps is better, but it takes longer.\n",
        "num_steps = 100#@param {type:\"integer\"}\n",
        "\n",
        "clip_search(text, seed, autoseed, num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX08_ZPZkL5U"
      },
      "source": [
        "# Download\n",
        "\n",
        "Download the found positions inside the latent space (.npz files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y0hUw3fkUp7"
      },
      "source": [
        "!zip -qr -0 /content/images.zip /content/images\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/images.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0pv_AU_OK0r"
      },
      "source": [
        "#Related Collab Notebooks and resources\n",
        "\n",
        "Search inside faces and other latent spaces: [StyleGAN3+CLIP Online](https://replicate.com/ouhenio/stylegan3-clip) /\n",
        "[StyleGAN3+CLIP Notebook](https://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2BCLIP.ipynb)\n",
        "\n",
        "Find a face inside the latent space and modify with text descriptions:  [StyleClip online](https://replicate.com/orpatashnik/styleclip) / [Styleclip notebooks at Github](https://github.com/orpatashnik/StyleCLIP)\n",
        "\n",
        "Search inside a more general latent space: [VQGAN+CLIP online](https://huggingface.co/spaces/multimodalart/vqgan) / [VQGAN+CLIP notebook](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP%28Updated%29.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTHr_I7cOIH_"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Taller Estampa https://tallerestampa.com / https://github.com/estampa\n",
        "\n",
        "### Based on\n",
        "Based on notebooks of [pbaylies fork](https://github.com/pbaylies/stylegan2-ada-pytorch) of [dvschultz Stylegan2-ADA Pytorch fork](https://github.com/dvschultz/stylegan2-ada-pytorch)"
      ]
    }
  ]
}