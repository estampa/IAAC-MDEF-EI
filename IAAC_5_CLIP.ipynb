{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entitled-exploration"
      },
      "source": [
        "# Image analysis and search using CLIP\n",
        "\n",
        "[OpenAI CLIP Model](https://github.com/openai/CLIP) is an embeddings model that is able to work with both text and images.\n",
        "\n",
        "We will work with a dataset of 25k images from [Unsplash Lite Dataset](https://unsplash.com/data).\n",
        "\n",
        "## Outline\n",
        "\n",
        "1.   Install libraries and download data\n",
        "2.   Search for clusters of similar images\n",
        "3.   Search images by text or another image\n",
        "\n"
      ],
      "id": "entitled-exploration"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "acoustic-pastor"
      },
      "outputs": [],
      "source": [
        "#@markdown ▶ Install libraries and load model\n",
        "\n",
        "!pip install -q sentence_transformers\n",
        "!pip install -q mediapy\n",
        "!pip install -q gradio\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from PIL import Image\n",
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import zipfile\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as IPImage\n",
        "import os\n",
        "import gradio as gr\n",
        "from tqdm.autonotebook import tqdm\n",
        "import mediapy as media\n",
        "\n",
        "#First, we load the respective CLIP model\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n"
      ],
      "id": "acoustic-pastor"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RRN0GBfKaUx"
      },
      "source": [
        "**Google drive**\n",
        "\n",
        "Only required if you are going to use your own images.\n",
        "\n"
      ],
      "id": "8RRN0GBfKaUx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlvE_TV6KKQl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "QlvE_TV6KKQl"
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Select if you want to use your own images or Unsplash dataset\n",
        "use_own_dataset = False # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Specify the path to your images on Drive\n",
        "dataset_path = '' # @param {type:\"string\"}\n",
        "\n",
        "img_folder = 'photos/' if not use_own_dataset else dataset_path"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sph7XKuNPZNo"
      },
      "id": "sph7XKuNPZNo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "major-injury"
      },
      "outputs": [],
      "source": [
        "#@markdown ▶ Download images\n",
        "\n",
        "#@markdown Only required if you don't use your own images\n",
        "\n",
        "# Next, we get about 25k images from Unsplash\n",
        "if not use_own_dataset and (not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0):\n",
        "    os.makedirs(img_folder, exist_ok=True)\n",
        "\n",
        "    photo_filename = 'unsplash-25k-photos.zip'\n",
        "    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n",
        "        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n",
        "\n",
        "    #Extract all images\n",
        "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
        "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
        "            zf.extract(member, img_folder)\n"
      ],
      "id": "major-injury"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "earned-mapping"
      },
      "outputs": [],
      "source": [
        "#@markdown ▶ Download precomputed embeddings or compute your own embeddings\n",
        "\n",
        "# Now, we need to compute the embeddings\n",
        "# To speed things up, we destribute pre-computed embeddings\n",
        "# Otherwise you can also encode the images yourself.\n",
        "# To encode an image, you can use the following code:\n",
        "# from PIL import Image\n",
        "# img_emb = model.encode(Image.open(filepath))\n",
        "\n",
        "import os\n",
        "\n",
        "if not use_own_dataset:\n",
        "    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n",
        "    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n",
        "        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n",
        "\n",
        "    with open(emb_filename, 'rb') as fIn:\n",
        "        img_names, img_emb = pickle.load(fIn)\n",
        "    print(\"Images:\", len(img_names))\n",
        "else:\n",
        "    emb_filename = 'embeddings.pkl'\n",
        "    emb_path = os.path.join(img_folder, emb_filename)\n",
        "    print(emb_path)\n",
        "\n",
        "    if os.path.exists(emb_path):\n",
        "      with open(emb_path, 'rb') as fIn:\n",
        "        img_names, img_emb = pickle.load(fIn)\n",
        "      print(\"Images:\", len(img_names))\n",
        "    else:\n",
        "      img_names = list(glob.glob(os.path.join(img_folder, '*.jpg')) + glob.glob(os.path.join(img_folder, '*.png')))\n",
        "      print(\"Images:\", len(img_names))\n",
        "      print(img_names)\n",
        "      img_emb = model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "      with open(emb_path, 'wb') as handle:\n",
        "        pickle.dump([img_names, img_emb], handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "id": "earned-mapping"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "equipped-script"
      },
      "outputs": [],
      "source": [
        "#@markdown ▶ Declare needed functions\n",
        "\n",
        "# We have implemented our own, efficient method\n",
        "# to find high density regions in vector space\n",
        "def community_detection(embeddings, threshold, min_community_size=10, init_max_size=1000):\n",
        "    \"\"\"\n",
        "    Function for Fast Community Detection\n",
        "\n",
        "    Finds in the embeddings all communities, i.e. embeddings that are close (closer than threshold).\n",
        "\n",
        "    Returns only communities that are larger than min_community_size. The communities are returned\n",
        "    in decreasing order. The first element in each list is the central point in the community.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute cosine similarity scores\n",
        "    cos_scores = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "    # Minimum size for a community\n",
        "    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)\n",
        "\n",
        "    # Filter for rows >= min_threshold\n",
        "    extracted_communities = []\n",
        "    for i in range(len(top_k_values)):\n",
        "        if top_k_values[i][-1] >= threshold:\n",
        "            new_cluster = []\n",
        "\n",
        "            # Only check top k most similar entries\n",
        "            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=True)\n",
        "            top_idx_large = top_idx_large.tolist()\n",
        "            top_val_large = top_val_large.tolist()\n",
        "\n",
        "            if top_val_large[-1] < threshold:\n",
        "                for idx, val in zip(top_idx_large, top_val_large):\n",
        "                    if val < threshold:\n",
        "                        break\n",
        "\n",
        "                    new_cluster.append(idx)\n",
        "            else:\n",
        "                # Iterate over all entries (slow)\n",
        "                for idx, val in enumerate(cos_scores[i].tolist()):\n",
        "                    if val >= threshold:\n",
        "                        new_cluster.append(idx)\n",
        "\n",
        "            extracted_communities.append(new_cluster)\n",
        "\n",
        "    # Largest cluster first\n",
        "    extracted_communities = sorted(extracted_communities, key=lambda x: len(x), reverse=True)\n",
        "\n",
        "    # Step 2) Remove overlapping communities\n",
        "    unique_communities = []\n",
        "    extracted_ids = set()\n",
        "\n",
        "    for community in extracted_communities:\n",
        "        add_cluster = True\n",
        "        for idx in community:\n",
        "            if idx in extracted_ids:\n",
        "                add_cluster = False\n",
        "                break\n",
        "\n",
        "        if add_cluster:\n",
        "            unique_communities.append(community)\n",
        "            for idx in community:\n",
        "                extracted_ids.add(idx)\n",
        "\n",
        "    return unique_communities\n",
        "\n",
        "# Next, we define a search function.\n",
        "def search(query, k=3):\n",
        "    # First, we encode the query (which can either be an image or a text string)\n",
        "    query_emb = model.encode([query], convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "    # Then, we use the util.semantic_search function, which computes the cosine-similarity\n",
        "    # between the query embedding and all image embeddings.\n",
        "    # It then returns the top_k highest ranked images, which we output\n",
        "    hits = util.semantic_search(query_emb, img_emb, top_k=k)[0]\n",
        "\n",
        "    output = []\n",
        "    for hit in hits:\n",
        "        image_path = os.path.join(img_folder, img_names[hit['corpus_id']])\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        output.append((image, f\"{hit['score']:.2f}\"))\n",
        "\n",
        "    return output"
      ],
      "id": "equipped-script"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oriental-shower",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ▶ Cluster the images and show samples from the 10 largest clusters\n",
        "\n",
        "# Now we run the clustering algorithm\n",
        "# With the threshold parameter, we define at which threshold we identify\n",
        "# two images as similar. Set the threshold lower, and you will get larger clusters which have\n",
        "# less similar images in it (e.g. black cat images vs. cat images vs. animal images).\n",
        "# With min_community_size, we define that we only want to have clusters of a certain minimal size\n",
        "clusters = community_detection(img_emb, threshold=0.9, min_community_size=10)\n",
        "\n",
        "num_clusters = 10\n",
        "num_images = 5\n",
        "\n",
        "images = []\n",
        "\n",
        "# Now we output the first 10 (largest) clusters\n",
        "for cluster in clusters[0:num_clusters]:\n",
        "\n",
        "    #Output 3 images\n",
        "    for idx in cluster[0:num_images]:\n",
        "        image_path = os.path.join(img_folder, img_names[idx])\n",
        "        images.append(Image.open(image_path))\n",
        "\n",
        "media.show_images(images, height=256, columns=5)"
      ],
      "id": "oriental-shower"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  ▶  Search the dataset by text or image\n",
        "\n",
        "def search_fn(input_file, input_text, number_of_results):\n",
        "    if input_file is not None:\n",
        "        return search(Image.open(input_file), k=number_of_results)\n",
        "    else:\n",
        "        return search(input_text, k=number_of_results)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Column():\n",
        "    input_file = gr.File(file_count=\"single\", file_types=[\".jpg\", \".png\"], label=\"Search image\")\n",
        "    input_text = gr.Textbox(label=\"Search text\")\n",
        "    number_of_results = gr.Number(label=\"Number of results\", minimum=1, value=3, maximum=10)\n",
        "    search_button = gr.Button(value=\"Search\")\n",
        "\n",
        "  with gr.Column():\n",
        "    gallery = gr.Gallery(label=\"Results\", show_label=True, columns=[3], object_fit=\"contain\", height=\"auto\")\n",
        "\n",
        "  search_button.click(search_fn, [input_file, input_text, number_of_results], [gallery])\n",
        "\n",
        "demo.launch(quiet=True, debug=False, height=768)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KMLjolAU0jsk"
      },
      "id": "KMLjolAU0jsk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tgbkOxbkf8B"
      },
      "source": [
        "# Finalizing\n",
        "\n",
        "When you finish working you have to remember to **stop the runtime**, because there is a time limit and to avoid wasting resources. To stop the runtime click Manage Sessions on the Runtime menu. Once the dialog opens click terminate on the current runtime.\n",
        "\n",
        "> But when you stop the runtime everything you have not saved is ⚠ **lost** ⚠, so be sure to **download** everything you want to keep before stopping it.\n"
      ],
      "id": "9tgbkOxbkf8B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNi8VV-wJDFi"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Taller Estampa https://tallerestampa.com / https://github.com/estampa\n",
        "\n",
        "### Based on\n",
        "\n",
        "[Sentence Transformers](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/image-search)\n"
      ],
      "id": "kNi8VV-wJDFi"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}