{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1yUcSgd27m"
      },
      "source": [
        "# Running a neural network using a library\n",
        "\n",
        "This continues what we did in the first notebook because we will continue using [Huggingface Transformers](https://github.com/huggingface/transformers). We could also use  [Huggingface Diffusers](https://github.com/huggingface/diffusers) that work with image generation and uses the same workflow.\n",
        "\n",
        "We will try to understand more the code that we used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install the library."
      ],
      "metadata": {
        "id": "yxqDfdomsA6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "SvOI4TDZJMKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines"
      ],
      "metadata": {
        "id": "0Cv2G1iQMoEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This library can run different tasks, that they organize in what they call [pipelines](https://huggingface.co/docs/transformers/pipeline_tutorial), and they are based on tasks or input/output format, like DepthEstimation or TextToAudio ([list](https://huggingface.co/docs/transformers/main_classes/pipelines)).\n",
        "\n",
        "We can create a pipeline with the task name\n",
        "\n",
        "```\n",
        "transcriber = pipeline(task=\"automatic-speech-recognition\")\n",
        "```\n",
        "or by selecting the model\n",
        "```\n",
        "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
        "```"
      ],
      "metadata": {
        "id": "hI1KV6BjKhkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "captioner = pipeline(model=\"Salesforce/blip-image-captioning-large\")"
      ],
      "metadata": {
        "id": "93AW3QCgKg3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once created we can give it an input and it will process it and give us an output."
      ],
      "metadata": {
        "id": "TQwX9i8-MJAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")"
      ],
      "metadata": {
        "id": "AQTL7AAoNAq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4pnQh9yMTre"
      },
      "source": [
        "The output format varies with the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "StkjTo2jM9zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also provide some parameters to the processing, they vary with the pipeline. In this case we change the maximum ouptut length (not very useful in this case)."
      ],
      "metadata": {
        "id": "I4ud5M-ENNKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\", max_new_tokens=8)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "MSVtWrRjNYm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n",
        "\n",
        "But we can also search for a specific model and use it directly without a piepline. The code is more complex, but we can have more control.\n",
        "\n",
        "We can search for a model here https://huggingface.co/models and usually they provide the code needed.\n",
        "\n",
        "In this example we are going to use the DPT model for Depth Estimation https://huggingface.co/Intel/dpt-large.\n"
      ],
      "metadata": {
        "id": "AjPq66_8MwwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
        "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
        "\n",
        "# prepare image for the model\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predicted_depth = outputs.predicted_depth\n",
        "\n",
        "# interpolate to original size\n",
        "prediction = torch.nn.functional.interpolate(\n",
        "    predicted_depth.unsqueeze(1),\n",
        "    size=image.size[::-1],\n",
        "    mode=\"bicubic\",\n",
        "    align_corners=False,\n",
        ")\n",
        "\n",
        "# visualize the prediction\n",
        "output = prediction.squeeze().cpu().numpy()\n",
        "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
        "depth = Image.fromarray(formatted)"
      ],
      "metadata": {
        "id": "CNvHVZG8N72O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We display the ouput"
      ],
      "metadata": {
        "id": "Se_2e-w0QUv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(depth)"
      ],
      "metadata": {
        "id": "qXUiwl5KPpte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tgbkOxbkf8B"
      },
      "source": [
        "# Finalizing\n",
        "\n",
        "When you finish working you have to remember to **stop the runtime**, because there is a time limit and to avoid wasting resources. To stop the runtime click Manage Sessions on the Runtime menu. Once the dialog opens click terminate on the current runtime.\n",
        "\n",
        "> But when you stop the runtime everything you have not saved is ⚠ **lost** ⚠, so be sure to **download** everything you want to keep before stopping it.\n"
      ]
    }
  ]
}